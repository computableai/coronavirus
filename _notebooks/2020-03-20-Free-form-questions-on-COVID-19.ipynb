{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free form questioning on COVID-19 dataset \n",
    "\n",
    ">We will use universal sentence encoder to encode text from COVID-19 dataset and use to answer queries\n",
    "\n",
    "- Based on the paper at: https://arxiv.org/abs/1803.11175\n",
    "\n",
    "- Dataset available at: https://pages.semanticscholar.org/coronavirus-research\n",
    "\n",
    "- By Dattaraj J Rao (Persistent Systems) - https://www.linkedin.com/in/dattarajrao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use the Sentence Transformers library\n",
    "\n",
    "Approach is to encode relevant text corpus from COVID-19 dataset and then match the question embedding with this to fiond top 3 matching answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/32/e3d405806ea525fd74c2c79164c3f7bc0b0b9811f27990484c6d6874c76f/sentence-transformers-0.2.5.1.tar.gz (52kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.3MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting transformers==2.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /opt/miniconda3/lib/python3.7/site-packages (from sentence-transformers) (4.36.1)\n",
      "Collecting torch>=1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/72/0282449efe6e8a7ab6354ac990b8275bd8c881dcbf95b3ef0a041da3897b/torch-1.4.0-cp37-none-macosx_10_9_x86_64.whl (81.1MB)\n",
      "\u001b[K     |████████████████████████████████| 81.1MB 54.3MB/s eta 0:00:01     |███████████                     | 27.6MB 38.1MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting numpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/14/6d7c914dac1cb2b596d2adace4aa4574d20c0789780f1339d535e69e271f/numpy-1.18.2-cp37-cp37m-macosx_10_9_x86_64.whl (15.1MB)\n",
      "\u001b[K     |████████████████████████████████| 15.1MB 15.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-learn in /Users/hiromi/.local/lib/python3.7/site-packages (from sentence-transformers) (0.21.3)\n",
      "Collecting scipy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/7a/ae480be23b768910a9327c33517ced4623ba88dc035f9ce0206657c353a9/scipy-1.4.1-cp37-cp37m-macosx_10_6_intel.whl (28.4MB)\n",
      "\u001b[K     |████████████████████████████████| 28.4MB 17.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 66.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/76/8ac7f467617b9cfbafcef3c76df6f22b15de654a62bea719792b00a83195/regex-2020.2.20.tar.gz (681kB)\n",
      "\u001b[K     |████████████████████████████████| 686kB 35.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /opt/miniconda3/lib/python3.7/site-packages (from transformers==2.3.0->sentence-transformers) (2.22.0)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 64.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/56/2e6cfc364c4760b85adab40cb38d91e7ce67d6b2745a2e1aa1497c776fe1/sentencepiece-0.1.85-cp37-cp37m-macosx_10_6_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 54.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/dc/7747cca8223bb62b8c2cc7aa65f860a9f0d454fbf8566e6da2d61e27fdcd/boto3-1.12.25-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 69.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/hiromi/.local/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/miniconda3/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/miniconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/miniconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/miniconda3/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: click in /Users/hiromi/.local/lib/python3.7/site-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 32.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /Users/hiromi/.local/lib/python3.7/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.4)\n",
      "Collecting botocore<1.16.0,>=1.15.25\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/00/476ae0cc6410c198c30ac397b2c3dc4cba9033ac80741f923510514a328c/botocore-1.15.25-py2.py3-none-any.whl (6.0MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0MB 60.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /Users/hiromi/.local/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.25->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /Users/hiromi/.local/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.25->boto3->transformers==2.3.0->sentence-transformers) (2.8.0)\n",
      "Building wheels for collected packages: sentence-transformers, nltk, regex, sacremoses\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5.1-cp37-none-any.whl size=67079 sha256=395f340545f3877db62dc241d40d30f6cbbd42b1d9937a7183cd6863dcabad31\n",
      "  Stored in directory: /Users/hiromi/Library/Caches/pip/wheels/22/ca/b4/7ca542b411730a8840f8e090df2ddacffa1c4dd9f209684c19\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449908 sha256=a4ead38ec905ba64ed896b6b0d4d9ef6d1fe63da7511592ee3cb1a42d36b06ab\n",
      "  Stored in directory: /Users/hiromi/Library/Caches/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2020.2.20-cp37-cp37m-macosx_10_9_x86_64.whl size=292221 sha256=bb0f57b2c09a389cdd8a99c2415b2ab8ade6fa01f309ad26397b13cd46f77a51\n",
      "  Stored in directory: /Users/hiromi/Library/Caches/pip/wheels/22/4a/99/3a20d265a6c7a186cc9903a4d45e29d0c8323c344e307b161e\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp37-none-any.whl size=884628 sha256=40512c7ffbfcb799c3e9de54fa4f9fdf1e1eab255242303373856f32cb69522d\n",
      "  Stored in directory: /Users/hiromi/Library/Caches/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sentence-transformers nltk regex sacremoses\n",
      "\u001b[31mERROR: awscli 1.16.304 has requirement botocore==1.13.40, but you'll have botocore 1.15.25 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: awscli 1.16.304 has requirement s3transfer<0.3.0,>=0.2.0, but you'll have s3transfer 0.3.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, regex, sacremoses, sentencepiece, botocore, s3transfer, boto3, transformers, torch, scipy, nltk, sentence-transformers\n",
      "  Found existing installation: botocore 1.13.40\n",
      "    Uninstalling botocore-1.13.40:\n",
      "      Successfully uninstalled botocore-1.13.40\n",
      "  Found existing installation: s3transfer 0.2.1\n",
      "    Uninstalling s3transfer-0.2.1:\n",
      "      Successfully uninstalled s3transfer-0.2.1\n",
      "Successfully installed boto3-1.12.25 botocore-1.15.25 nltk-3.4.5 numpy-1.18.2 regex-2020.2.20 s3transfer-0.3.3 sacremoses-0.0.38 scipy-1.4.1 sentence-transformers-0.2.5.1 sentencepiece-0.1.85 torch-1.4.0 transformers-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset, create embeddings vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/CORD-19-research-challenge/2020-03-13/noncomm_use_subset/noncomm_use_subset/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b32fd702c731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mJSON_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/CORD-19-research-challenge/2020-03-13/noncomm_use_subset/noncomm_use_subset/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mjson_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpos_json\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos_json\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJSON_PATH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpos_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/CORD-19-research-challenge/2020-03-13/noncomm_use_subset/noncomm_use_subset/'"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "JSON_PATH = 'CORD-19-research-challenge/2020-03-13/noncomm_use_subset/noncomm_use_subset/'\n",
    "\n",
    "json_files = [pos_json for pos_json in os.listdir(JSON_PATH) if pos_json.endswith('.json')]\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# loop through the files\n",
    "for jfile in json_files[::]:\n",
    "    # for each file open it and read as json\n",
    "    with open(os.path.join(JSON_PATH, jfile)) as json_file:\n",
    "        covid_json = json.load(json_file)\n",
    "        # read abstract\n",
    "        for item in covid_json['abstract']:\n",
    "            corpus.append(item['text'])\n",
    "        # read body text\n",
    "        for item in covid_json['body_text']:\n",
    "            corpus.append(item['text'])\n",
    "            \n",
    "print(\"Corpus size = %d\"%(len(corpus)))\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy\n",
    "\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "corpus_embeddings = embedder.encode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def ask_question(query):\n",
    "    queries = [query]\n",
    "    query_embeddings = embedder.encode(queries)\n",
    "\n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    closest_n = 5\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        \n",
    "        # get the closest answers\n",
    "        for idx, distance in results[0:closest_n]:\n",
    "            print(\"\\n\\n======================\\n\\n\")        \n",
    "            print(\"ANSWER = \\n\", corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Does smoking or pre-existing pulmonary disease increase risk of COVID-19?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question('Does smoking or pre-existing pulmonary disease increase risk of COVID-19?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Are neonates and pregnant women ar greater risk of COVID-19?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question('Are neonates and pregnant women ar greater risk of COVID-19?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question3: Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question('Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question('Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question('Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask your own question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ask_question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9b1e6dac2f68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mask_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ask_question' is not defined"
     ]
    }
   ],
   "source": [
    "ask_question(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
